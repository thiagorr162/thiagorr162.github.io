<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://thiagorr162.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://thiagorr162.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-31T20:15:45+00:00</updated><id>https://thiagorr162.github.io/feed.xml</id><title type="html">blank</title><subtitle>My personal website </subtitle><entry><title type="html">A Clever Trick to Prove the Divergence of the Harmonic Series</title><link href="https://thiagorr162.github.io/posts/2024/12/harmonic_series_divergence/" rel="alternate" type="text/html" title="A Clever Trick to Prove the Divergence of the Harmonic Series"/><published>2024-12-21T00:00:00+00:00</published><updated>2024-12-21T00:00:00+00:00</updated><id>https://thiagorr162.github.io/posts/2024/12/harmonic_series_divergence</id><content type="html" xml:base="https://thiagorr162.github.io/posts/2024/12/harmonic_series_divergence/"><![CDATA[<p>The inequality \(e^x \geq 1+x\) offers a neat way to show that the harmonic series diverges. Applying it for \(x = \frac{1}{n}\) we get:</p> \[e^{1/n} \geq 1 + 1/n = \frac{n+1}{n}.\] <p>Repeating this idea \(n\) times gives:</p> \[e^{1 + 1/2 + 1/3 + \dots + 1/n} \geq \frac{2}{1} \cdot \frac{3}{2} \cdot \dots \cdot \frac{n+1}{n} = n+1.\] <p>Taking \(log\) both sides shows that the harmonic sum \(H_n := 1 + 1/2 + \dots + 1/n\) satisfies:</p> \[H_n \geq \log(n+1),\] <p>proving that the harmonic series diverges.</p> <hr/> <p><em>Inspired by Ragib Zaman’s comment on <a href="https://math.stackexchange.com/questions/691807/proofs-of-am-gm-inequality">Math Stack Exchange</a>, June 12, 2014.</em></p>]]></content><author><name></name></author><category term="inequality"/><category term="harmonic series"/><summary type="html"><![CDATA[The inequality \(e^x \geq 1+x\) offers a neat way to show that the harmonic series diverges. Applying it for \(x = \frac{1}{n}\) we get:]]></summary></entry><entry><title type="html">The coolest proof you are going to learn today</title><link href="https://thiagorr162.github.io/posts/2024/07/decoupling_inequality/" rel="alternate" type="text/html" title="The coolest proof you are going to learn today"/><published>2024-07-26T00:00:00+00:00</published><updated>2024-07-26T00:00:00+00:00</updated><id>https://thiagorr162.github.io/posts/2024/07/decoupling_theorem</id><content type="html" xml:base="https://thiagorr162.github.io/posts/2024/07/decoupling_inequality/"><![CDATA[<p>We are interested in studying quadratic forms of the type:</p> \[\langle X,AX \rangle = X^TAX,\] <p>where \(X \in \mathbb{R}^d\) is a random vector with i.i.d. coordinates and \(A\) is a \(d \times d\) matrix. Such quadratic forms are called chaos in probability. For simplicity, we will assume that \(X_i\) have zero mean and unit variance.</p> <p>Our main goal is to establish the following result: Given a convex function \(F: \mathbb{R} \to \mathbb{R}\),</p> \[F(X^TA_*X) \leq F(4X^T A X'),\] <p>where \(X'\) is an independent copy of \(X\) and \(A_*\) is the matrix \(A\) with 0’s on its diagonal.</p> <p>The advantage of the RHS is that it is linear in \(X\) rather than quadratic. This implies, for example, that one can write</p> \[\sum_{i=1}^d \left(\sum_{j=1}^d a_{ij}X_j'\right)X_i = \sum_{i=1}^d c_iX_i,\] <p>which allows us to use results for sums of linear i.i.d. variables.</p> <p>Our goal is to make the \(X\) on the right side of \(X^TAX\) independent of the \(X\) on the left side. Note that if we fix a subset \(I \subset [d]\), then the sum</p> \[\sum_{i \in I, j \in I^c} a_{ij} X_i X_j\] <p>satisfies the idea of the left part (\(X_i\)) being independent of the right part (\(X_j\)).</p> <p>The first cool trick in this proof is to notice that for a fixed vector \(x \in \mathbb{R}\):</p> \[\begin{align*} x^TA_*x &amp;= \sum_{i \neq j} a_{ij} x_i x_j \\ &amp;= 4\mathbb{E}_\delta\left[ \sum_{i \neq j} \delta_i(1-\delta_j)a_{ij}x_ix_j\right] \\ &amp;= 4\mathbb{E}_I\left[\sum_{i \in I, j \in I^c} a_{ij} x_i x_j\right], \end{align*}\] <p>where \(\delta_i = 1\) with probability \(1/2\) and the same for \(\delta_i=0\). The \(4\) in this expression appears because this is the expectation of \(\delta(1-\delta)\).</p> <p>Applying the expectation over \(X\) on both sides, we get</p> \[\mathbb{E}_X\left[F(X^TA_*X)\right] \leq \mathbb{E}_X\left[F\left(\mathbb{E}_I\left[4\sum_{i \in I, j \in I^c} a_{ij} X_i X_j\right]\right)\right].\] <p>By Jensen and Fubini’s theorems:</p> \[\mathbb{E}_X\left[F(X^TA_*X)\right] \leq \mathbb{E}_I\mathbb{E}_X\left[F\left(4\sum_{i \in I, j \in I^c} a_{ij} X_i X_j\right)\right].\] <p>Now, be prepared for the second trick! Note that if \(\mathbb{E}(Z) \geq a\) for some random variable \(Z\) and \(a \in \mathbb{R}\), then there must exist a realization \(Z(\omega) \geq a\) (think about this!). Hence, there exists a realization \(I\), such that</p> \[\mathbb{E}_X\left[F(X^TA_*X)\right] \leq \mathbb{E}_X\left[F\left(4\sum_{i \in I, j \in I^c} a_{ij} X_i X_j'\right)\right].\] <p>From now on, we will work with this fixed \(I\). Note that we have already changed the right term \(X_j\) to its independent copy \(X_j'\). We can do this because the terms \(i\) and \(j\) are in complementary sets, making \(X_i\) and \(X_j\) independent.</p> <p>Now, we need to show that</p> \[\mathbb{E}_X\left[F\left(4\sum_{i \in I, j \in I^c} a_{ij} X_i X_j'\right)\right] \leq \mathbb{E}_X\left[F\left(4\sum_{i,j=1}^n a_{ij} X_i X_j'\right)\right] .\] <p>To do this, write the sum in right expression as \(Y+Z_1+Z_2\), where \(Y\) is the sum over \(I \times I^c\) (the sum in left expression), \(Z_1\) is the sum over \(I \times I\), and \(Z_2\) is the sum over \(I^c \times [n]\) (make a drawing!).</p> <p>Now, condition on all variables appearing in the \(Y\) term, that is, \(i\in I\) and \(j\in I^c\). This makes \(Y\) fixed, \(Z_1\) with randomness only over \(j \in I\), and \(Z_2\) only with randomness over \(i \in I^c\) and \(j \in I\). This makes \(Z_1\) and \(Z_2\) zero mean random variables, so</p> \[F(4Y + 0 + 0) = F(4Y + \mathbb{E}_{Z_1,Z_2}(4Z_1 + 4Z_2)).\] <p>Using Jensen again, we have</p> \[F(4Y) = \mathbb{E}_{Z_1,Z_2}F(4Y + 4Z_1 + 4Z_2).\] <p>Finally, taking the expectation over all random variables, we get</p> \[\mathbb{E}(F(4Y)) \leq (\star),\] <p>concluding the proof!</p> <h3 id="references">References</h3> <ul> <li><a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">High-Dimensional Probability</a> - R. Vershynin</li> </ul>]]></content><author><name></name></author><category term="inequality"/><category term="concentration"/><category term="probability"/><category term="decoupling"/><summary type="html"><![CDATA[We are interested in studying quadratic forms of the type:]]></summary></entry><entry><title type="html">Introduction to isoperimetric inequalities and concentration of measure</title><link href="https://thiagorr162.github.io/posts/2024/07/isoperimetric_concentration_1/" rel="alternate" type="text/html" title="Introduction to isoperimetric inequalities and concentration of measure"/><published>2024-07-13T00:00:00+00:00</published><updated>2024-07-13T00:00:00+00:00</updated><id>https://thiagorr162.github.io/posts/2024/07/isoperimetric_concentration</id><content type="html" xml:base="https://thiagorr162.github.io/posts/2024/07/isoperimetric_concentration_1/"><![CDATA[<p>Consider a metric space \((\mathcal{X},d)\) and a 1-Lipschitz function \(f:\mathcal{X}\to \mathbb{R}\). Given a probability \(P\) and a random variable \(X\sim P\), we are interested in bounding the following quantity:</p> \[P\left(f(X)\geq M(f) + t\right)\quad (\star),\] <p>where \(M(f)\) is the median of the function \(f\) given \(P\), that is, \(M(f)\) is a real number such that</p> \[P\left( f(X)\geq M(f) \right)\geq 1/2\quad \textrm{and} \quad P\left( f(X)\leq M(f) \right)\geq 1/2.\] <p>First, given \(t\in\mathbb{R}\), define the t-blowup of \(A\) as</p> \[A_t := \{x\in\mathcal{X}: d(x,A)&lt;t\}\] <p>Note that, if \(B=\left\lbrace x\in\mathcal{X}: f(x)\leq M(f)\right\rbrace\), then for any \(x\in B\) and \(y\in \mathcal{X}\) such that \(d(x,y)&lt; t\), since \(f\) is 1-Lipschitz,</p> \[f(y)-f(x)\leq d(x,y) \Rightarrow f(y)\leq t + f(x)\Rightarrow f(y)&lt; t + M(f).\] <p>That is, we have that \(B_t\subset \left\lbrace x\in\mathcal{X}: f(x)&lt; t + M(f)\right\rbrace.\) Taking the complement, we conclude that:</p> \[P\left(f(X)\geq M(f) + t \right)\leq P(\mathcal{X}\backslash B_t) = P\left(d(X,B)\geq t\right).\] <p>Therefore, we reduce the problem of estimating \((\star)\) to estimating the quantity \(P(d(X,B)\geq t)\) where \(B\) is an event with probability at least \(1/2\) by the definition of \(M(F)\). Indeed, we can upper bound the above expression as</p> \[P\left(f(X)\geq M(f) + t \right)\leq P(\mathcal{X}\backslash B_t) \leq \alpha(t)\] <p>where</p> \[\alpha(t):= \sup_{A\subset \mathcal{X}, P(A)\geq 1/2} P(d(X,A)\geq t).\] <p>At first glance, the function \(\alpha(t)\) seems harder to bound than the probability \((\star)\). However, note that studying \(\alpha(t)\) is equivalent to study the quantity</p> \[1 - \inf_{A\subset \mathcal{X}, P(A)\geq 1/2} P(d(X,A)&lt; t).\] <p>That is, we are interested in studying maximal big sets (probability larger than 1/2), with small surface area (small blowup probability). But this is exactly what isoperimetric results try to understand!</p> <h3 id="references">References</h3> <ul> <li><a href="https://academic.oup.com/book/26549">Concentration Inequalities</a> - S. Boucheron, G. Lugosi, P. Massart</li> <li><a href="http://wwwusers.di.uniroma1.it/~ale/Papers/master.pdf">Concentration of Measure for the Analysis of Randomized Algorithms</a> - D. Dubhashi, A. Panconesi</li> </ul>]]></content><author><name></name></author><category term="isoperimetric"/><category term="concentration"/><category term="probability"/><summary type="html"><![CDATA[Consider a metric space \((\mathcal{X},d)\) and a 1-Lipschitz function \(f:\mathcal{X}\to \mathbb{R}\). Given a probability \(P\) and a random variable \(X\sim P\), we are interested in bounding the following quantity:]]></summary></entry></feed>